from __future__ import annotations

import math
from typing import Any, Dict, List, Literal, Optional, Tuple

import pandas as pd

from debate_mas.protocol import EtfCandidate, SkillResult
from debate_mas.skills.base import BaseFinanceSkill, SkillContext

from .mapping import (
    GUARDRAIL_BUCKETS,
    GUARDRAIL_NOTE,
    INDUSTRY_FUZZY_MAP,
    THEME_KEYWORDS_MAP,
)
from .ontology import get_concept_meta

Mode = Literal["ontology_mapping", "industry_frequency", "guardrail_pool", "user_custom"]

class SkillHandler(BaseFinanceSkill):
    """
    [Hunter] ä¸»é¢˜æŒ–æ˜æœº - Recall æ¨¡å— (The Theme Miner)
    
    è®¾è®¡:
    - Recall (å¬å›): ä»æµ·é‡ ETF ä¸­æ‰¾å‡ºä¸â€œæ¦‚å¿µ/è¡Œä¸š/æ”¿ç­–â€ç›¸å…³çš„æ ‡çš„ã€‚
    - Evidence (è¯æ®): ä¸ä»…ç»™ä»£ç ï¼Œè¿˜è¦ç»™â€œå«æƒé‡â€ (Match Count) å’Œâ€œæ”¿ç­–å¼ºåº¦â€ (Policy Strength)ã€‚
    - No Rank (æ— æ’åº): æ’åºäº¤ç»™ quantitative_sniperï¼Œè¿™é‡Œåªåšâ€œæµ·é€‰â€ã€‚
    """
    SKILL_NAME = "theme_miner"
    TABLE_ETF = "etf_basic"
    TABLE_GOV = "govcn"
    OUTPUT_TYPE = "EtfCandidateList"

    def execute(self, 
                ctx: SkillContext, 
                mode: Mode = "ontology_mapping",
                keyword: str = "",
                event_text: str = "",
                days: int = 30, 
                top_k: int = 10,
                top_industries: int = 3,
                guardrail_buckets: Optional[List[str]] = None,  
                per_bucket_k: int = 3,
                ) -> SkillResult:
        # ==========================================================
        # 1. åŸºç¡€æ•°æ®å‡†å¤‡ (Data Preparation)
        # ==========================================================
        df_etf = ctx.dossier.get_table(self.TABLE_ETF)
        if df_etf is None:
            return SkillResult.fail("æ¡ˆå·ä¸­ç¼ºå¤± 'etf_basic' åŸºç¡€ä¿¡æ¯è¡¨ã€‚")

        df_etf = self._norm_cols(df_etf)
        df_etf = self._apply_etf_setup_date_filter(df_etf, ctx.ref_date)

        # è¯†åˆ«å…³é”®åˆ—ï¼ˆcode/nameï¼‰
        self._code_col, self._name_col = self._infer_etf_cols(df_etf)
        if not self._code_col or not self._name_col:
            return SkillResult.fail(f"etf_basic è¡¨ç¼ºå°‘å¿…è¦çš„ code æˆ– name åˆ—ã€‚ç°æœ‰: {list(df_etf.columns)}")

        # ==========================================================
        # 2. æ¨¡å¼è·¯ç”± (Mode Dispatch)
        # ==========================================================
        # Mode A: ç»“æ„å…œåº• (Guardrail Pool) - çº¯è§„åˆ™ï¼Œä¸ä¾èµ–æ”¿ç­–è¡¨
        if mode == "guardrail_pool":
            return self._run_guardrail_pool(
                df_etf=df_etf,
                top_k=top_k,
                guardrail_buckets=guardrail_buckets,
                per_bucket_k=per_bucket_k,
                ctx=ctx,
            )
        
        # ä»¥ä¸‹æ¨¡å¼éœ€è¦æ”¿ç­–è¡¨
        df_gov = ctx.dossier.get_table(self.TABLE_GOV)
        if df_gov is None:
            return SkillResult.fail("æ¡ˆå·ä¸­ç¼ºå¤± 'govcn' æ”¿ç­–è¡¨ã€‚")

        df_gov = self.apply_date_filter(df_gov, ctx.ref_date)
        if df_gov.empty:
            return SkillResult.fail(f"æˆªæ­¢ {ctx.ref_date} æ— å¯ç”¨æ”¿ç­–æ•°æ®ã€‚")
        df_gov = self._norm_cols(df_gov)

        # Mode B: ä¸»é¢˜æ˜ å°„ (Ontology Mapping) - æ ¸å¿ƒæ¨¡å¼
        if mode == "ontology_mapping":
            if not keyword:
                return SkillResult.fail("ontology_mapping æ¨¡å¼å¿…é¡»æä¾› keyword å‚æ•°ã€‚")
            return self._run_ontology_mapping(df_gov, df_etf, keyword=keyword, days=days, top_k=top_k, ctx=ctx)

        # Mode C: è¡Œä¸šè¯é¢‘ (Industry Frequency) - æ•°æ®é©±åŠ¨
        if mode == "industry_frequency":
            return self._run_industry_frequency(df_gov, df_etf, days=days, top_k=top_k, top_industries=top_industries, ctx=ctx)

        # Mode D: è‡ªå®šä¹‰ (User Custom) - ç»ƒä¹ è€…æ¥å£
        if mode == "user_custom":
            return self._user_custom_logic(df_gov, df_etf, keyword=keyword, days=days, top_k=top_k)

        return SkillResult.fail(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}")

    def _ok_candidates(
        self,
        ctx: SkillContext,
        out: List[EtfCandidate],
        insight: str,
        meta: Dict[str, Any],
    ) -> SkillResult:
        data = {
            "type": self.OUTPUT_TYPE,
            "items": [c.model_dump() for c in out],
            "meta": {
                "ref_date": ctx.ref_date,
                "agent_role": ctx.agent_role,
                **(meta or {}),
            },
        }
        return SkillResult.ok(data=data, insight=insight)

    # =========================================================================
    # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ (Core Business Logic)
    # =========================================================================
    def _run_ontology_mapping(self, df_gov: pd.DataFrame, df_etf: pd.DataFrame, keyword: str, days: int, top_k: int, ctx: SkillContext) -> SkillResult:
        """
        [ç­–ç•¥] çŸ¥è¯†å›¾è°±æ˜ å°„ï¼šä»â€œå®è§‚æ¦‚å¿µâ€æ¨å¯¼åˆ°â€œå…·ä½“ ETFâ€
        
        Step 1: æ¦‚å¿µæ‰©å±• (Concept Expansion) -> ä» ontology.yaml æŸ¥è¡¨
        Step 2: æ”¿ç­–éªŒè¯ (Policy Verification) -> å» govcn æ‰¾è¯æ®
        Step 3: æ ‡çš„å¬å› (ETF Recall) -> å» etf_basic æ¨¡ç³ŠåŒ¹é…
        """
        # --- Step 1: Concept Expansion ---
        meta = get_concept_meta(keyword)
        if not meta.get("found", False):
            return SkillResult.fail(f"çŸ¥è¯†åº“æœªæ”¶å½• '{keyword}'ï¼Œè¯·å…ˆåœ¨ references/ontology.yaml ä¸­è¡¥å…… aliases/expands_toã€‚")

        std_theme = str(meta.get("name", keyword))
        expansions = [str(x) for x in (meta.get("expansions", []) or []) if str(x).strip()]
        static_weight = float(meta.get("weight", 1.0))

        # --- Step 2: Policy Verification ---
        matched_docs, evidence_str, policy_strength = self._search_documents(df_gov, keyword=std_theme, days=days)
        if std_theme != keyword and len(matched_docs) == 0:
             matched_docs, evidence_str, policy_strength = self._search_documents(df_gov, keyword=keyword, days=days)

        # --- Step 3: ETF Recall ---
        terms = self._uniq_keep([keyword, std_theme] + expansions)

        hits = []
        for t in terms:
            h = self._match_etfs(df_etf, t)
            if not h.empty:
                h = h.copy()
                h["_match_term"] = t 
                hits.append(h)

        if not hits:
            return SkillResult.fail(f"ä¸»é¢˜å‘½ä¸­ï¼Œä½†æœªåœ¨ etf_basic ä¸­å¬å›åˆ° ETFï¼ˆå°è¯•è¯: {terms[:6]}ï¼‰ã€‚")

        # --- Step 4: Aggregation & Scoring ---
        m = pd.concat(hits, ignore_index=True)
        m[self._code_col] = m[self._code_col].astype(str)

        agg = m.groupby(self._code_col).agg(
            etf_name=(self._name_col, "first"),
            hit_terms=("_match_term", "nunique"), 
        ).reset_index()

        agg["score"] = (10.0 + 5.0 * agg["hit_terms"]).clip(lower=1.0, upper=60.0)
        agg = agg.sort_values(["hit_terms", self._code_col], ascending=[False, True]).head(int(top_k))

        # --- Step 5: Wrap Result ---
        out: List[EtfCandidate] = []
        for _, r in agg.iterrows():
            code = str(r[self._code_col])
            reason = f"ä¸»é¢˜:{std_theme}(w={static_weight}) | å‘½åå‘½ä¸­:{int(r['hit_terms'])}é¡¹ | è¯æ®:{evidence_str if evidence_str else 'æ— '}"
            
            out.append(EtfCandidate(
                symbol=code,
                score=float(round(r["score"], 2)),
                reason=reason,
                source_skill=self.SKILL_NAME,
                extra={
                    "mode": "ontology_mapping",
                    "theme": std_theme,
                    "static_weight": static_weight,
                    "search_terms": terms[:10],
                    "policy_docs": int(len(matched_docs)) if matched_docs is not None else 0,
                    "policy_strength": float(policy_strength),
                },
            ))

        insight = f"[ontology_mapping] ä¸»é¢˜'{std_theme}' å¬å› {len(out)} åª ETFï¼Œæ”¿ç­–è¯æ® {len(matched_docs)} æ¡ã€‚"
        return self._ok_candidates(
            ctx=ctx,
            out=out,
            insight=insight,
            meta={
                "mode": "ontology_mapping",
                "keyword": keyword,
                "std_theme": std_theme,
                "days": int(days),
                "top_k": int(top_k),
                "policy_docs": int(len(matched_docs)) if matched_docs is not None else 0,
                "policy_strength": float(policy_strength),
            },
        )

    def _run_guardrail_pool(self, df_etf: pd.DataFrame, top_k: int, guardrail_buckets: Optional[List[str]], per_bucket_k: int, ctx: SkillContext) -> SkillResult:
        """
        [ç­–ç•¥] ç»“æ„å…œåº•ï¼šä¿è¯ç»„åˆçš„â€œéª¨æ¶â€å®Œæ•´ (Bond, Gold, Cash...)
        ä¸çœ‹æ–°é—»ï¼Œåªçœ‹é…ç½®éœ€æ±‚ã€‚
        """
        buckets = guardrail_buckets or (GUARDRAIL_BUCKETS[:] if GUARDRAIL_BUCKETS else [])
        if not buckets:
            return self._ok_candidates(
                ctx=ctx,
                out=[],
                insight="[guardrail_pool] æœªé…ç½® GUARDRAIL_BUCKETSï¼Œè·³è¿‡ã€‚",
                meta={"mode": "guardrail_pool", "buckets": []},
            )

        top_k = int(max(1, top_k))
        per_bucket_k = int(max(1, per_bucket_k))

        rows: List[Dict[str, Any]] = []

        for b in buckets:
            b = str(b).strip()
            terms = [str(x).strip() for x in (THEME_KEYWORDS_MAP.get(b, []) or []) if str(x).strip()]
            
            cnt = 0
            seen = set()

            for t in terms:
                h = self._match_etfs(df_etf, t)
                if h.empty: 
                    continue
                
                for _, r in h.iterrows():
                    code = str(r[self._code_col])
                    if code in seen: 
                        continue
                    seen.add(code)
                    
                    rows.append({
                        "code": code,
                        "etf_name": str(r.get(self._name_col, "")),
                        "bucket": b,
                        "hit_term": t,
                    })
                    cnt += 1
                    if cnt >= per_bucket_k: break
                if cnt >= per_bucket_k: break

        if not rows:
            return SkillResult.fail("[guardrail_pool] æœªå¬å›åˆ°ä»»ä½• ETF (Mapping é…ç½®å¯èƒ½ä¸ºç©º)ã€‚")

        m = pd.DataFrame(rows)
        agg = (
            m.groupby("code")
            .agg(
                etf_name=("etf_name", "first"),
                buckets=("bucket", lambda xs: sorted(set(xs))),
                buckets_n=("bucket", "nunique"),
                hit_terms_n=("hit_term", "nunique"),
            )
            .reset_index()
        )

        agg["score"] = (12.0 + 3.0 * agg["buckets_n"] + 2.0 * agg["hit_terms_n"]).clip(1.0, 60.0)
        agg = agg.sort_values(["buckets_n", "hit_terms_n", "code"], ascending=[False, False, True]).head(top_k)

        out: List[EtfCandidate] = []
        for _, r in agg.iterrows():
            reason = f"ç»“æ„å…œåº•:{','.join(r['buckets'])} | {GUARDRAIL_NOTE}"
            out.append(
                EtfCandidate(
                    symbol=str(r["code"]),
                    score=float(round(r["score"], 2)),
                    reason=reason,
                    source_skill=self.SKILL_NAME,
                    extra={
                        "mode": "guardrail_pool",
                        "buckets": r["buckets"],
                        "note": GUARDRAIL_NOTE,
                    },
                )
            )

        insight = f"[guardrail_pool] Buckets={buckets} äº§å‡º {len(out)} åª ETFã€‚"
        return self._ok_candidates(
            ctx=ctx,
            out=out,
            insight=insight,
            meta={
                "mode": "guardrail_pool",
                "buckets": buckets,
                "top_k": int(top_k),
                "per_bucket_k": int(per_bucket_k),
            },
        )
    
    def _run_industry_frequency(self, df_gov: pd.DataFrame, df_etf: pd.DataFrame, days: int, top_k: int, top_industries: int,ctx: SkillContext) -> SkillResult:
        """[ç­–ç•¥] è¡Œä¸šè¯é¢‘ï¼šæ•°æ®é©±åŠ¨çš„â€œçƒ­ç‚¹å‘ç°â€"""
        if "industry_name" not in df_gov.columns:
            return SkillResult.fail("govcn è¡¨ç¼ºå°‘ industry_name å­—æ®µã€‚")

        # 1. åˆ‡ç‰‡ä¸ç»Ÿè®¡
        d = self._slice_lookback(df_gov, days)
        if d.empty: return SkillResult.fail(f"çª—å£å†…æ— æ•°æ® (days={days})ã€‚")

        s = d["industry_name"].astype(str).map(self._clean_industry_name)
        s = s[s.str.len() > 0]
        if s.empty: return SkillResult.fail("industry_name æ¸…æ´—åä¸ºç©ºã€‚")

        # 2. ç»Ÿè®¡ Top N è¡Œä¸š
        vc = s.value_counts()
        top_n = int(max(1, min(int(top_industries), len(vc))))
        top_inds = vc.head(top_n) 

        # 3. æ˜ å°„ ETF

        hits: List[pd.DataFrame] = []
        for ind, freq in top_inds.items():
            terms = self._industry_terms(ind)
            for t in terms:
                h = self._match_etfs(df_etf, t)
                if h.empty:
                    continue
                h = h.copy()
                h["_industry"] = ind
                h["_freq"] = int(freq)
                h["_match_term"] = t
                hits.append(h)

        if not hits:
            return SkillResult.fail(f"Topè¡Œä¸š {list(top_inds.index)} æœªèƒ½å¬å› ETFã€‚")

        # 4. èšåˆæ‰“åˆ†
        m = pd.concat(hits, ignore_index=True)
        m[self._code_col] = m[self._code_col].astype(str)

        agg = (
            m.groupby(self._code_col)
            .agg(
                etf_name=(self._name_col, "first"),
                industry=("_industry", "first"),
                freq=("_freq", "max"),
                hit_terms=("_match_term", "nunique"),
            )
            .reset_index()
        )

        agg["score"] = (10.0 + 2.0 * agg["freq"] + 3.0 * agg["hit_terms"]).clip(1.0, 60.0)
        agg = agg.sort_values(["freq", "hit_terms", self._code_col], ascending=[False, False, True]).head(int(top_k))

        # 5. å°è£…
        out: List[EtfCandidate] = []
        for _, r in agg.iterrows():
            reason = f"è¡Œä¸šé«˜é¢‘:{r['industry']}({r['freq']}) | çª—å£:{days}å¤©"
            out.append(EtfCandidate(
                symbol=str(r[self._code_col]),
                score=float(round(r["score"], 2)),
                reason=reason,
                source_skill=self.SKILL_NAME,
                extra={
                    "mode": "industry_frequency",
                    "industry": r["industry"],
                    "freq": int(r["freq"]),
                    "lookback_days": int(days),
                },
            ))

        insight = f"[industry_frequency] Topè¡Œä¸š={list(top_inds.index)} å¬å› {len(out)} åª ETFã€‚"
        return self._ok_candidates(
            ctx=ctx,
            out=out,
            insight=insight,
            meta={
                "mode": "industry_frequency",
                "days": int(days),
                "top_k": int(top_k),
                "top_industries": int(top_industries),
                "top_industry_list": list(top_inds.index),
            },
        )

    # ================= ğŸš€ ç»ƒä¹ è€…æ‰©å±•æ¥å£ (Student Lab) =================
    def _user_custom_logic(self, df_gov: pd.DataFrame, df_etf: pd.DataFrame, keyword: str, days: int, top_k: int) -> SkillResult:
        """
        [TODO] ç»ƒä¹ è€…è¯·åœ¨æ­¤å¤„ç¼–å†™ä½ çš„è‡ªå®šä¹‰å¬å›é€»è¾‘
        è¾“å…¥: df_gov ä¸ºå·²é˜²æœªæ¥åˆ‡ç‰‡åçš„ govcnï¼›df_etf ä¸ºå·²åˆ—æ ‡å‡†åŒ–ä¸”å·²è¿‡æ»¤ setup_date<=ref_date çš„ etf_basic
        è¾“å‡º: å¿…é¡»è¿”å› SkillResult(data.type="EtfCandidateList")ï¼Œitems å†… symbol/score/reason/extra
        éªŒæ”¶: mode="user_custom" æ—¶èƒ½ç¨³å®šå¬å›ï¼›extra è‡³å°‘å†™å…¥ mode/keyword/lookback_days ç­‰å¤ç°ä¿¡æ¯
        """
        return SkillResult.fail("è‡ªå®šä¹‰å¬å›é€»è¾‘å°šæœªå®ç°ï¼šè¯·åœ¨ _user_custom_logic å†…è¡¥å…¨ä»£ç ã€‚")

    # ================= å·¥å…·å‡½æ•° (Helper Methods) =================
    def _norm_cols(self, df: pd.DataFrame) -> pd.DataFrame:
        return df.rename(columns=lambda x: str(x).strip().lower())

    def _infer_etf_cols(self, df_etf: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:
        """æ™ºèƒ½æ¨æ–­ Code å’Œ Name åˆ—å"""
        code_col = next((c for c in df_etf.columns if c in ["code", "symbol", "ts_code", "masterfundcode"]), None)
        name_col = next((c for c in df_etf.columns if c in ["cname", "csname", "name", "extname"]), None)
        return code_col, name_col
    
    def _apply_etf_setup_date_filter(self, df_etf: pd.DataFrame, ref_date: str) -> pd.DataFrame:
        """é˜²æœªæ¥: å‰”é™¤ ref_date ä¹‹åæˆç«‹çš„ ETF"""
        d = df_etf.copy()
        ref = pd.to_datetime(ref_date, errors="coerce")
        if pd.isna(ref): return d

        date_col = next((c for c in ["setup_date", "list_date", "pub_date", "base_date", "date"] if c in d.columns), None)
        if not date_col: return d

        d[date_col] = pd.to_datetime(d[date_col], errors="coerce")
        d = d.dropna(subset=[date_col])
        return d[d[date_col] <= ref].copy()
    
    def _slice_lookback(self, df: pd.DataFrame, days: int) -> pd.DataFrame:
        """æ—¶é—´çª—å£åˆ‡ç‰‡"""
        d = df.copy()
        date_col = next((c for c in d.columns if c in ["date", "pub_date", "time"]), None)
        if not date_col: return d
        
        d[date_col] = pd.to_datetime(d[date_col], errors="coerce")
        d = d.dropna(subset=[date_col])
        if d.empty: return d
        
        latest_dt = d[date_col].max()
        cutoff = latest_dt - pd.Timedelta(days=int(days))
        return d[d[date_col] >= cutoff]

    def _uniq_keep(self, xs: List[str]) -> List[str]:
        """ä¿æŒé¡ºåºå»é‡"""
        out: List[str] = []
        for x in xs:
            x = str(x).strip()
            if x and x not in out: out.append(x)
        return out

    def _clean_industry_name(self, s: str) -> str:
        """è¡Œä¸šåæ¸…æ´— (e.g. 'æ±½è½¦åˆ¶é€ è¡Œä¸š' -> 'æ±½è½¦åˆ¶é€ ')"""
        s = str(s).strip()
        if not s or s.lower() in ["nan", "none"]: return ""
        if s in ["ç»¼åˆ", "å…¶ä»–æœåŠ¡ä¸š", "å…¶ä»–åˆ¶é€ ä¸š", "å…¶ä»–é‡‘èä¸š"]: return ""
        for suf in ["è¡Œä¸š", "äº§ä¸š", "é¢†åŸŸ", "ç›¸å…³"]:
            if s.endswith(suf): s = s[: -len(suf)]
        return s.strip()

    def _industry_terms(self, industry_name: str) -> List[str]:
        """è¡Œä¸šå -> æœç´¢è¯åº“ (åˆ©ç”¨ mappings.yaml è¿›è¡Œæ¨¡ç³Šæ‰©å±•)"""
        ind = str(industry_name).strip()
        if not ind: return []

        base = ind
        for suf in ["åˆ¶é€ ä¸š", "æœåŠ¡ä¸š", "åŠ å·¥ä¸š", "ä¾›åº”ä¸š", "ç®¡ç†ä¸š", "è¿è¾“ä¸š", "å¼€é‡‡ä¸š", "é‡‡é€‰ä¸š", "å»ºç­‘ä¸š", "åˆ©ç”¨ä¸š", "ä¸š"]:
            if base.endswith(suf): base = base[: -len(suf)]
        base = base.strip()

        extras: List[str] = []
        if base in INDUSTRY_FUZZY_MAP:
            extras.extend(INDUSTRY_FUZZY_MAP.get(base, []) or [])
        else:
            for k, vs in (INDUSTRY_FUZZY_MAP or {}).items():
                k = str(k).strip()
                if not k: continue
                if k in base or base in k:
                    extras.extend(vs or [])

        return self._uniq_keep([ind, base] + [str(x).strip() for x in extras if str(x).strip()])
    
    def _search_documents(self, df: pd.DataFrame, keyword: str, days: int) -> Tuple[pd.DataFrame, str, float]:
        """[å·¥å…·] æœç´¢æ”¿ç­–æ–‡æ¡£ (æ”¯æŒæ—¶é—´è¡°å‡è¯„åˆ†)"""
        text_cols = [c for c in df.columns if c in ["title", "content", "context", "industry_name"]]
        if not text_cols: return pd.DataFrame(), "", 0.0

        kw = str(keyword).strip()
        if not kw: return pd.DataFrame(), "", 0.0

        mask = pd.Series(False, index=df.index)
        for col in text_cols:
            mask |= df[col].astype(str).str.contains(kw, na=False)

        matched = df[mask].copy()
        if matched.empty: return matched, "", 0.0

        # æ—¶é—´è¿‡æ»¤
        date_col = next((c for c in matched.columns if c in ["date", "pub_date", "time"]), None)
        if date_col:
            try:
                matched[date_col] = pd.to_datetime(matched[date_col], errors="coerce")
                matched = matched.dropna(subset=[date_col])
                if not matched.empty:
                    anchor = matched[date_col].max()
                    cutoff = anchor - pd.Timedelta(days=int(days))
                    matched = matched[matched[date_col] >= cutoff].sort_values(date_col, ascending=False)
            except Exception: pass

        # æ”¿ç­–å¼ºåº¦è®¡ç®— (è¡°å‡æ¨¡å‹)
        half_life_days = 30.0
        lam = math.log(2.0) / half_life_days
        policy_strength = float(len(matched))

        if date_col and not matched.empty:
            try:
                latest_dt = matched[date_col].max()
                age_days = (latest_dt - matched[date_col]).dt.days.clip(lower=0)
                weights = (-lam * age_days).apply(math.exp)
                policy_strength = float(weights.sum())
            except Exception: pass

        # è¯æ®æ‘˜è¦
        show_col = "title" if "title" in text_cols else text_cols[0]
        evidence_list = matched[show_col].head(3).tolist()
        evidence_str = " | ".join([str(e)[:30] + "..." for e in evidence_list])

        return matched, evidence_str, policy_strength

    def _match_etfs(self, df: pd.DataFrame, keyword: str) -> pd.DataFrame:
        """[å·¥å…·] ç®€å•çš„æ¨¡ç³ŠåŒ¹é… (Contains)"""
        kw = str(keyword).strip()
        if not self._name_col or not kw: return pd.DataFrame()
        mask = df[self._name_col].astype(str).str.contains(kw, na=False)
        return df[mask].copy()